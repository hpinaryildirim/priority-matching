{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyNrVZz5Ce9n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import ast\n",
        "from typing import Optional, Dict, List\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r3d6MDG3CpON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"\")"
      ],
      "metadata": {
        "id": "ldNwoNTdCpQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "tP66zKotDzSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get aggregated results"
      ],
      "metadata": {
        "id": "-QE6GFOM1PbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ratio(num, den):\n",
        "    return round(num / den, 2) if den else 0\n",
        "\n",
        "def get_metrics(df: pd.DataFrame, entity_type: str, priority_col: str = \"all\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute completion, cancellation, abandonment ratios and distance metrics\n",
        "    overall and optionally broken down by priority value.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): Input dataframe containing at least [\"status\", \"assignment_distance\", priority columns].\n",
        "    entity_type (str): Entity type to calculate metrics for 'passenger' or 'driver'\n",
        "    priority_col (str): \"all\" (default) to calculate across all priority levels or one of [\"priority_0.05\", \"priority_0.1\", \"priority_0.2\", \"priority_0.3\"].\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(df)\n",
        "    if total == 0:\n",
        "        return {\"error\": \"Empty dataframe\"}\n",
        "\n",
        "    results = []\n",
        "    results.append((\"all\", \"all\", \"total\", total))\n",
        "\n",
        "\n",
        "    for status in [\"Completed\", \"Abandoned\"]:\n",
        "        subset = df[df[\"status\"] == status][\"assignment_distance\"]\n",
        "        results.append((\"all\", \"all\", f\"avg_assignment_distance_{status.lower()}\", round(subset.mean(), 2)))\n",
        "        results.append((\"all\", \"all\", f\"max_assignment_distance_{status.lower()}\", round(subset.max(), 2)))\n",
        "\n",
        "    if entity_type == \"passenger\":\n",
        "      status_list = [\"Completed\", \"Cancelled\", \"Abandoned\"]\n",
        "\n",
        "      for status in status_list:\n",
        "        count = (df[\"status\"] == status).sum()\n",
        "        results.append((\"all\", \"all\", f\"{status.lower()}_count\", count))\n",
        "        results.append((\"all\", \"all\", f\"ratio_{status.lower()}\", get_ratio(count, total)))\n",
        "\n",
        "    if entity_type == \"driver\":\n",
        "      completed = (df[\"status\"] == \"Completed\").sum()\n",
        "      results.append((\"all\", \"all\", \"completed\", completed))\n",
        "      results.append((\"all\", \"all\", \"completed_ratio\", get_ratio(completed, completed)))\n",
        "\n",
        "      abandoned = (df[\"status\"] == \"Abandoned\").sum()\n",
        "      results.append((\"all\", \"all\", \"abandoned\", abandoned))\n",
        "      results.append((\"all\", \"all\", \"abandoned_ratio\", get_ratio(abandoned, abandoned)))\n",
        "\n",
        "      matched = completed + abandoned\n",
        "      results.append((\"all\", \"all\", \"matched\", matched))\n",
        "      results.append((\"all\", \"all\", \"matched_ratio\", get_ratio(matched, matched)))\n",
        "\n",
        "\n",
        "    #select priority columns\n",
        "    priority_values = [0.05, 0.1, 0.2, 0.3]\n",
        "    if priority_col == \"all\":\n",
        "        selected = [f\"priority_{t}\" for t in priority_values]\n",
        "    elif priority_col.startswith(\"priority_\"):\n",
        "        selected = [priority_col]\n",
        "    else:\n",
        "        selected = []\n",
        "\n",
        "    #calculate metrics for priority\n",
        "    for col in selected:\n",
        "        total_scope = len(df)\n",
        "        for val, label in [(1, \"priority\"), (0, \"nonpriority\")]:\n",
        "            subset = df[df[col] == val]\n",
        "            sub_total = len(subset)\n",
        "            results.append((col, label, \"total\", sub_total))\n",
        "            results.append((col, label, \"ratio\", get_ratio(sub_total, total_scope)))\n",
        "\n",
        "            for status in [\"Completed\", \"Abandoned\"]:\n",
        "                  dist = subset.loc[subset[\"status\"] == status, \"assignment_distance\"]\n",
        "                  results.append((col, label, f\"avg_assignment_distance_{status.lower()}\", round(dist.mean(), 2)))\n",
        "                  results.append((col, label, f\"max_assignment_distance_{status.lower()}\", round(dist.max(), 2)))\n",
        "\n",
        "            if entity_type == \"passenger\":\n",
        "              for status in status_list:\n",
        "                  count = (subset[\"status\"] == status).sum()\n",
        "                  results.append((col, label, f\"{status.lower()}_count\", count))\n",
        "                  results.append((col, label, f\"ratio_{status.lower()}\", get_ratio(count, sub_total)))\n",
        "\n",
        "            if entity_type == \"driver\":\n",
        "                comp = (subset[\"status\"] == 'Completed').sum()\n",
        "                results.append((col, label, \"completed\", comp))\n",
        "                results.append((col, label, \"completed_ratio\",  get_ratio(comp, completed)))\n",
        "\n",
        "                abnd = (subset[\"status\"] == 'Abandoned').sum()\n",
        "                results.append((col, label, \"abandoned\", abnd))\n",
        "                results.append((col, label, \"abandoned_ratio\",  get_ratio(abnd, abandoned)))\n",
        "\n",
        "                mtch = comp + abnd\n",
        "                results.append((col, label, \"matched\", mtch))\n",
        "                results.append((col, label, \"matched_ratio\", get_ratio(mtch, matched)))\n",
        "\n",
        "    df_long = pd.DataFrame(results, columns=[\"scope\", \"group\", \"metric\", \"value\"])\n",
        "    df_wide = df_long.pivot_table(index=[\"scope\", \"group\"], columns=\"metric\", values=\"value\" ).reset_index()\n",
        "\n",
        "    return df_wide"
      ],
      "metadata": {
        "id": "GKqgAw-QFvF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_clean(path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load a simulation outcome CSV file, filter arrival_time > 10, and drop rows with missing 'status'.\n",
        "\n",
        "    Args:\n",
        "        path (str): Full path to the CSV file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(path, converters={\"loc\": ast.literal_eval})\n",
        "    df = df[df[\"arrival_time\"] > 10].copy()\n",
        "    df.dropna(subset=[\"status\"], inplace=True)\n",
        "    return df\n",
        "\n",
        "def process_folder(folder: str, save_dir: Optional[str] = None) -> Dict[str, Optional[pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Process a folder of passenger and driver CSVs, compute metrics, and return results.\n",
        "\n",
        "    Args:\n",
        "        folder (str): Folder name containing simulation outcome CSV files.\n",
        "        save_dir (str): Directory to save outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Processing folder: {folder}\")\n",
        "    include_priorities = priority_map.get(folder)\n",
        "    print(f\"Include priorities: {include_priorities}\")\n",
        "\n",
        "    overall_results = {\"passenger\": [], \"driver\": []}\n",
        "\n",
        "    for setting in range(1, 109):\n",
        "        print(f\"Parameter Setting: {setting}\")\n",
        "        setting_results = {\"passenger\": [], \"driver\": []}\n",
        "\n",
        "        for j in [1, 2, 3]:\n",
        "            passenger_file = os.path.join(os.getcwd(), folder, f\"result_pdf_{setting:03d}_{j:03d}.csv\")\n",
        "            driver_file = os.path.join(os.getcwd(), folder, f\"result_ddf_{setting:03d}_{j:03d}.csv\")\n",
        "\n",
        "            passengers_df = load_and_clean(passenger_file)\n",
        "            drivers_df = load_and_clean(driver_file)\n",
        "\n",
        "            if \"driver\" in folder or folder in [\"base\", \"nearest_neighbor\"]:\n",
        "                drivers_df = drivers_df.merge(passengers_df[[\"id\", \"assignment_distance\"]], how=\"left\", left_on=\"assigned_passenger\", right_on = \"id\") #to get assignment distances\n",
        "                drivers_metrics = get_metrics(drivers_df, \"driver\", include_priorities)\n",
        "                setting_results[\"driver\"].append(drivers_metrics)\n",
        "\n",
        "                passengers_metrics = get_metrics(passengers_df, \"passenger\", include_priorities)\n",
        "                setting_results[\"passenger\"].append(passengers_metrics)\n",
        "\n",
        "            if \"passenger\" in folder or folder in [\"base\", \"nearest_neighbor\"]:\n",
        "                passengers_metrics = get_metrics(passengers_df, \"passenger\", include_priorities)\n",
        "                setting_results[\"passenger\"].append(passengers_metrics)\n",
        "\n",
        "        for entity in [\"passenger\", \"driver\"]:\n",
        "            if setting_results[entity]:\n",
        "                df_concat = pd.concat(setting_results[entity], ignore_index=True)\n",
        "                df_concat[\"setting\"] = setting\n",
        "                overall_results[entity].append(df_concat)\n",
        "\n",
        "    final_passenger_df = pd.concat(overall_results[\"passenger\"], ignore_index=True) if overall_results[\"passenger\"] else None\n",
        "    final_driver_df = pd.concat(overall_results[\"driver\"], ignore_index=True) if overall_results[\"driver\"] else None\n",
        "\n",
        "    if save_dir:\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        if final_passenger_df is not None:\n",
        "            final_passenger_df.to_csv(os.path.join(save_dir, f\"{folder}_passenger_results.csv\"), index=False)\n",
        "        if final_driver_df is not None:\n",
        "            final_driver_df.to_csv(os.path.join(save_dir, f\"{folder}_driver_results.csv\"), index=False)\n",
        "\n",
        "    return {\"passenger\": final_passenger_df, \"driver\": final_driver_df}\n"
      ],
      "metadata": {
        "id": "plU25Ln5kIJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_folders(folders: List[str], save_dir: Optional[str] = None) -> Dict[str, Dict[str, Optional[pd.DataFrame]]]:\n",
        "    \"\"\"\n",
        "    Process all folders in parallel and return a dictionary of results.\n",
        "\n",
        "    Args:\n",
        "    folders (List[str]): List of folder names to process.\n",
        "    save_dir (str): Directory to save outputs.\n",
        "    \"\"\"\n",
        "\n",
        "    results = {}\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        future_to_folder = {executor.submit(process_folder, folder, save_dir): folder for folder in folders}\n",
        "        for future in tqdm(as_completed(future_to_folder), total=len(folders), desc=\"Processing folders\"):\n",
        "            folder_name = future_to_folder[future]\n",
        "            try:\n",
        "                results[folder_name] = future.result()\n",
        "                print(f\"Completed folder: {folder_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing folder {folder_name}: {e}\")\n",
        "                results[folder_name] = {\"passenger\": None, \"driver\": None}\n",
        "    return results"
      ],
      "metadata": {
        "id": "tv-7KXe2r0WX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "priority_map = {\n",
        "    \"base\": \"all\",\n",
        "    \"nearest_neighbor\": \"all\",\n",
        "    \"passenger_priority_0.05\": \"priority_0.05\",\n",
        "    \"driver_priority_0.05\": \"priority_0.05\",\n",
        "    \"passenger_priority_0.1\": \"priority_0.1\",\n",
        "    \"driver_priority_0.1\": \"priority_0.1\",\n",
        "    \"passenger_priority_0.2\": \"priority_0.2\",\n",
        "    \"driver_priority_0.2\": \"priority_0.2\",\n",
        "    \"passenger_priority_0.3\": \"priority_0.3\",\n",
        "    \"driver_priority_0.3\": \"priority_0.3\"\n",
        "}\n",
        "folders = [\"base\", \"nearest_neighbor\", \"passenger_priority_0.05\", \"passenger_priority_0.1\", \"passenger_priority_0.2\", \"passenger_priority_0.3\", \"driver_priority_0.05\", \"driver_priority_0.1\", \"driver_priority_0.2\" , \"driver_priority_0.3\"]"
      ],
      "metadata": {
        "id": "JcLV6_NBJ206"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results = process_all_folders(folders, os.path.join(os.getcwd(), 'results'))"
      ],
      "metadata": {
        "id": "Z3S1-OROsAT8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze results"
      ],
      "metadata": {
        "id": "6lN0woi41I7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_df(df, scope, group):\n",
        "  filtered_df = df.copy()\n",
        "  filtered_df = filtered_df[(filtered_df['scope'] == scope) & (filtered_df['group'] == group)].groupby(['lambda_p', 'lambda_d_ratio', 'scope', 'group']).mean().round(2).reset_index()\n",
        "  return filtered_df\n",
        "\n",
        "def get_metric_comparison(df_list, algorithm_label_list, metric_list, scope_list):\n",
        "\n",
        "  all_results = []\n",
        "  metric_res = {}\n",
        "\n",
        "  for scope in scope_list:\n",
        "    for df, tspmp in zip(df_list, algorithm_label_list):\n",
        "      if scope in df['scope'].unique():\n",
        "        if scope == 'all':\n",
        "          df = filter_df(df, scope, 'all')\n",
        "          df[\"priority\"] = tspmp\n",
        "          df['scope'] = scope\n",
        "          all_results.append(df[[\"lambda_p\", \"lambda_d_ratio\", \"priority\", \"scope\"] + metric_list])\n",
        "        else:\n",
        "          df = filter_df(df, scope, 'priority')\n",
        "          df[\"priority\"] = tspmp\n",
        "          df['scope'] = scope\n",
        "          all_results.append(df[[\"lambda_p\", \"lambda_d_ratio\", \"priority\", \"scope\"] + metric_list])\n",
        "\n",
        "  results = pd.concat(all_results, ignore_index=True)\n",
        "\n",
        "  for v in metric_list:\n",
        "    metric_res[v] = results.pivot_table(index=[\"lambda_p\", \"lambda_d_ratio\"],\n",
        "                            columns=[\"priority\", \"scope\"],\n",
        "                            values=v)\n",
        "  return metric_res"
      ],
      "metadata": {
        "id": "hsbjdEt52EHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read parameter setting\n",
        "params_setting = pd.read_csv('param_set_list.csv', sep=\",\")"
      ],
      "metadata": {
        "id": "_XBIguQlsH5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_folder = 'results'"
      ],
      "metadata": {
        "id": "Fu3FYc06-sIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read result csv files\n",
        "passenger_results = {}\n",
        "driver_results = {}\n",
        "\n",
        "for result_file in os.listdir(os.path.join(os.getcwd(), results_folder)):\n",
        "  result_csv = pd.read_csv(os.path.join(os.getcwd(), results_folder, result_file), index_col=0)\n",
        "  result_csv.reset_index(drop=False, inplace=True)\n",
        "  result_csv = result_csv.merge(params_setting, how=\"left\", left_on=\"setting\", right_on=\"set_id\")\n",
        "  result_csv['lambda_d_ratio'] = result_csv['lambda_d'] / result_csv['lambda_p']\n",
        "  result_csv.drop([\"set_id\", \"setting\", \"passenger_patience_before\", \"driver_patience_before\"], axis=1, inplace=True)\n",
        "\n",
        "  if 'passenger' in result_file:\n",
        "    passenger_results[result_file[:-4]] = result_csv\n",
        "  elif 'driver' in result_file:\n",
        "    driver_results[result_file[:-4]] = result_csv"
      ],
      "metadata": {
        "id": "siWrfFkUdXPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Passenger priority results**"
      ],
      "metadata": {
        "id": "uUrNEXf3x_MI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare overall results"
      ],
      "metadata": {
        "id": "SWuebk_gWuYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_priority_keys = [\n",
        "    \"base_passenger_results\",\n",
        "    \"nearest_neighbor_passenger_results\",\n",
        "    \"passenger_priority_0.05_passenger_results\",\n",
        "    \"passenger_priority_0.1_passenger_results\",\n",
        "    \"passenger_priority_0.2_passenger_results\",\n",
        "    \"passenger_priority_0.3_passenger_results\",\n",
        "]\n",
        "\n",
        "passenger_priority_passenger_df_list = [pd.DataFrame(passenger_results[k]) for k in passenger_priority_keys]\n",
        "passenger_metrics = [\"ratio_completed\", \"ratio_cancelled\", \"ratio_abandoned\", \"avg_assignment_distance_completed\", \"avg_assignment_distance_abandoned\"]"
      ],
      "metadata": {
        "id": "upESOBxq6b_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BA vs. NN"
      ],
      "metadata": {
        "id": "haaHCK6J74Jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric_comparisons = get_metric_comparison(passenger_priority_passenger_df_list, ['BA', 'NN'], passenger_metrics, ['all'])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "E5iAaOg-FKK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in metric_comparisons:\n",
        "  print(k)\n",
        "  display(metric_comparisons[k])"
      ],
      "metadata": {
        "id": "AcQZSomiUuZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### BA vs. TSPMP overall performance"
      ],
      "metadata": {
        "id": "tuq5ih6t7-uO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_priority_keys = [\n",
        "    \"base_passenger_results\",\n",
        "    \"passenger_priority_0.05_passenger_results\",\n",
        "    \"passenger_priority_0.1_passenger_results\",\n",
        "    \"passenger_priority_0.2_passenger_results\",\n",
        "    \"passenger_priority_0.3_passenger_results\",\n",
        "]\n",
        "\n",
        "passenger_priority_passenger_df_list = [pd.DataFrame(passenger_results[k]) for k in passenger_priority_keys]\n",
        "passenger_metrics = [\"ratio_completed\", \"ratio_cancelled\", \"ratio_abandoned\", \"avg_assignment_distance_completed\", \"avg_assignment_distance_abandoned\"]"
      ],
      "metadata": {
        "id": "TZfVTXuH9tO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_comparisons = get_metric_comparison(passenger_priority_passenger_df_list, ['BA', 'TSPMP 05%', 'TSPMP 10%', 'TSPMP 20%', 'TSPMP 30%'], passenger_metrics, ['all'])"
      ],
      "metadata": {
        "id": "I71spl5j8N4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in metric_comparisons:\n",
        "  print(k)\n",
        "  display(metric_comparisons[k])"
      ],
      "metadata": {
        "id": "gXypnThzVKvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare priority results"
      ],
      "metadata": {
        "id": "Ifo8kS-OWzPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "passenger_priority_keys = [\n",
        "    \"base_passenger_results\",\n",
        "    \"passenger_priority_0.05_passenger_results\",\n",
        "    \"passenger_priority_0.1_passenger_results\",\n",
        "    \"passenger_priority_0.2_passenger_results\",\n",
        "    \"passenger_priority_0.3_passenger_results\",\n",
        "]\n",
        "\n",
        "passenger_priority_passenger_df_list = [pd.DataFrame(passenger_results[k]) for k in passenger_priority_keys]\n",
        "\n",
        "passenger_algorithm_list = ['BA', 'TSPMP 05%', 'TSPMP 10%', 'TSPMP 20%', 'TSPMP 30%']\n",
        "passenger_metrics = [\"ratio_completed\", \"ratio_cancelled\", \"ratio_abandoned\", \"avg_assignment_distance_completed\"]"
      ],
      "metadata": {
        "id": "v2QnT5Q6aG0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_comparisons = get_metric_comparison(passenger_priority_passenger_df_list, passenger_algorithm_list, passenger_metrics,  ['all', 'priority_0.05', 'priority_0.1', 'priority_0.2', 'priority_0.3'])"
      ],
      "metadata": {
        "id": "LWVutPvS_ytq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in metric_comparisons:\n",
        "  print(k)\n",
        "  display(metric_comparisons[k])"
      ],
      "metadata": {
        "id": "twSoWXgwWMeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Driver priority results**"
      ],
      "metadata": {
        "id": "ae3Vb-BCyF9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare overall results"
      ],
      "metadata": {
        "id": "OKe0KDgg4y40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "driver_priority_passenger_keys = [\n",
        "    \"base_passenger_results\",\n",
        "    \"driver_priority_0.05_passenger_results\",\n",
        "    \"driver_priority_0.1_passenger_results\",\n",
        "    \"driver_priority_0.2_passenger_results\",\n",
        "    \"driver_priority_0.3_passenger_results\",\n",
        "]\n",
        "\n",
        "driver_priority_passenger_df_list = [pd.DataFrame(passenger_results[k]) for k in driver_priority_passenger_keys]\n",
        "\n",
        "driver_algorithm_list = ['BA', 'TSPMD 05%', 'TSPMD 10%', 'TSPMD 20%', 'TSPMD 30%']\n",
        "driver_priority_passenger_metrics = [\"ratio_completed\", \"ratio_cancelled\", \"ratio_abandoned\", \"avg_assignment_distance_completed\", \"avg_assignment_distance_abandoned\"]"
      ],
      "metadata": {
        "id": "xsGabd6w_y2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_comparisons = get_metric_comparison(driver_priority_passenger_df_list, driver_algorithm_list, driver_priority_passenger_metrics,  ['all'])"
      ],
      "metadata": {
        "id": "oDn3VSnXEGpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in metric_comparisons:\n",
        "  print(k)\n",
        "  display(metric_comparisons[k])"
      ],
      "metadata": {
        "id": "fHTQ1Y6hX4S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver_priority_driver_keys = [\n",
        "    \"base_driver_results\",\n",
        "    \"driver_priority_0.05_driver_results\",\n",
        "    \"driver_priority_0.1_driver_results\",\n",
        "    \"driver_priority_0.2_driver_results\",\n",
        "    \"driver_priority_0.3_driver_results\",\n",
        "]\n",
        "\n",
        "driver_priority_driver_df_list = [pd.DataFrame(driver_results[k]) for k in driver_priority_driver_keys]\n",
        "\n",
        "driver_algorithm_list = ['BA', 'TSPMD 05%', 'TSPMD 10%', 'TSPMD 20%', 'TSPMD 30%']\n",
        "driver_priority_passenger_metrics = [\"matched_ratio\", \"completed_ratio\", \"abandoned_ratio\", \"avg_assignment_distance_completed\", \"avg_assignment_distance_abandoned\"]"
      ],
      "metadata": {
        "id": "rRSGsicU3wPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_comparisons = get_metric_comparison(driver_priority_driver_df_list, driver_algorithm_list, driver_priority_passenger_metrics,  ['priority_0.05', 'priority_0.1', 'priority_0.2', 'priority_0.3'])"
      ],
      "metadata": {
        "id": "aVhrPCzD3yUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in metric_comparisons:\n",
        "  print(k)\n",
        "  display(metric_comparisons[k])"
      ],
      "metadata": {
        "id": "nlq1b-gvYr_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DoCCPY8XcunD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SAdaxQsh0Wzy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}